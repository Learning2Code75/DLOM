{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c5dfc5",
   "metadata": {},
   "source": [
    "# Vid1 :\n",
    "# Vid2 : \n",
    "# Vid3 : text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdf245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf08c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/IMDBDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29839016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd1bed24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1329a8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1c134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5d3d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f76daee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing html tags \n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0dc0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production. the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(df['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a26b20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a264ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLs\n",
    "text1 = ' www.google.com'\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40d97b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcfc8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8513eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string.time\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for string.time\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install string.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69795621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations \n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e4c30e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuations(\".,/:;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "011c45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da532f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25912</th>\n",
       "      <td>new best friend is another entry in the steal ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43445</th>\n",
       "      <td>after highlander 2 which i am still in denial ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>less than 10 minutes into this film i wanted i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>this is an excellent example of what can be do...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10437</th>\n",
       "      <td>i was duped as well here i was expecting all s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "25912  new best friend is another entry in the steal ...  negative\n",
       "43445  after highlander 2 which i am still in denial ...  negative\n",
       "19338  less than 10 minutes into this film i wanted i...  negative\n",
       "18841  this is an excellent example of what can be do...  positive\n",
       "10437  i was duped as well here i was expecting all s...  negative"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c09950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat word treatment\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "def chat_word_treatment(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.lower() in abbreviations:\n",
    "            new_text.append(abbreviations[w.lower()])\n",
    "        else :\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9615fffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as soon as possible , sleeping bored and tired for your information'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_word_treatment(\"asap , zzz fyi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e24e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(chat_word_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "941b74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f076336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spelling correction\n",
    "from textblob import TextBlob\n",
    "textBlb = TextBlob('ceertain')\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "631e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        textBlb = TextBlob(w)\n",
    "        new_text.append(textBlb.correct().string)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6947b168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain certain'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_correct(\"ceertain ceertain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbd5e035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspell_correct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mspell_correct\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m      4\u001b[0m     textBlb \u001b[38;5;241m=\u001b[39m TextBlob(w)\n\u001b[0;32m----> 5\u001b[0m     new_text\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtextBlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(new_text)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:609\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m--> 609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:608\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m    609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/en/__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w]) \\\n\u001b[1;32m   1398\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w)) \\\n\u001b[0;32m-> 1399\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \\\n\u001b[1;32m   1400\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[1;32m   1401\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m   1402\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1367\u001b[0m, in \u001b[0;36mSpelling._edit1\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1364\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m   1365\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1366\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m-> 1367\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA]\n\u001b[1;32m   1368\u001b[0m )\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1367\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1364\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m   1365\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1366\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m-> 1367\u001b[0m     [\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA]\n\u001b[1;32m   1368\u001b[0m )\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(spell_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44bcc461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/neelchoksi19bce0990/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69bc6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else :\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2e6cf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     kuch toh'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"a the for she it kuch toh \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ece12cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m new_text\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m         new_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m :\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mline_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/tokenize/simple.py:137\u001b[0m, in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mline_tokenize\u001b[39m(text, blanklines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscard\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLineTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblanklines\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/tokenize/simple.py:113\u001b[0m, in \u001b[0;36mLineTokenizer.tokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m--> 113\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# If requested, strip off blank lines.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blanklines \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9dfb03d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#emoji handling\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memoji\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(emoji\u001b[38;5;241m.\u001b[39mdemojize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython is ðŸ”¥\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "#emoji handling\n",
    "import emoji\n",
    "print(emoji.demojize(\"Python is ðŸ”¥\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7df1959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ada', 'ad', 'fa', 'dfa', 'df!']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization \n",
    "sent1 = \"I ada ad fa dfa df!\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "121c98cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ada', 'ad', 'fa', 'dfa', 'df']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "tokens = re.findall(\"[\\w']+\",sent1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75141b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/neelchoksi19bce0990/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'ada', 'ad', 'fa', 'dfa', 'df', '!']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c033a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (3.2.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.4 MB 511 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (1.22.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459 kB 501 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (52.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.25.1)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (813 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813 kB 500 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.1.1)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.2 MB 573 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.2-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: srsly, wasabi, confection, blis, thinc, spacy\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.4.2\n",
      "    Uninstalling srsly-2.4.2:\n",
      "      Successfully uninstalled srsly-2.4.2\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 0.9.0\n",
      "    Uninstalling wasabi-0.9.0:\n",
      "      Successfully uninstalled wasabi-0.9.0\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 0.7.6\n",
      "    Uninstalling blis-0.7.6:\n",
      "      Successfully uninstalled blis-0.7.6\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.15\n",
      "    Uninstalling thinc-8.0.15:\n",
      "      Successfully uninstalled thinc-8.0.15\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.4\n",
      "    Uninstalling spacy-3.2.4:\n",
      "      Successfully uninstalled spacy-3.2.4\n",
      "Successfully installed blis-0.7.8 confection-0.0.2 spacy-3.4.1 srsly-2.4.4 thinc-8.1.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8e60841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.8 MB 530 kB/s eta 0:00:01     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11.7 MB 500 kB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.2)\n",
      "Requirement already satisfied: jinja2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.63.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (52.0.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.22.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.25.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/neelchoksi19bce0990/.local/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae7cde74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 108\n",
      "\u001b[2K\u001b[38;5;2mâœ” Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.4.1) =================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ spaCy installation:\n",
      "/home/neelchoksi19bce0990/.local/lib/python3.9/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                            \n",
      "en_core_web_sm   >=3.4.0,<3.5.0   \u001b[38;5;2m3.4.0\u001b[0m   \u001b[38;5;2mâœ”\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f72c5b86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(sent1)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc1:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 109\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(sent1)\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "728d9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming:\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "555c2be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7244388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5ab7f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walking', 'playing', 'eating', 'eat']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sent = \"walking playing eating eat\"\n",
    "sent_words = nltk.word_tokenize(sent)\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f44af04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking             walk                \n",
      "playing             play                \n",
      "eating              eat                 \n",
      "eat                 eat                 \n"
     ]
    }
   ],
   "source": [
    "for word in sent_words :\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86c110",
   "metadata": {},
   "source": [
    "# Vid 4 : Text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49321a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e87cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fef6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26639a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c8d9b85",
   "metadata": {},
   "source": [
    "# Vid 5 : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071dfb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ed5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e5043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc11a2a",
   "metadata": {},
   "source": [
    "# Vid 6 : TextClassif - ML Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd1302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd6817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b0acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57c239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256f838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164e0044",
   "metadata": {},
   "source": [
    "# PyData - ANN, LSTM, GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982380a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fb548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857a1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b123937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab218052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e6ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
