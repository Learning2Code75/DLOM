{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07f39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d2990",
   "metadata": {},
   "source": [
    "# Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac53e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../3_regClassif/senti_clientwise_try1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e82f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59094, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>I only rented this because i loved the first m...</td>\n",
       "      <td>This film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>'I don't understand. None of this makes any se...</td>\n",
       "      <td>Taylor Hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17393</td>\n",
       "      <td>I know that to include everything in the book,...</td>\n",
       "      <td>I don't know about the real Cobb but I got the...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17393</td>\n",
       "      <td>This is easily the worst, most offensive piece...</td>\n",
       "      <td>The film starts in the Long Island Kennel Club...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17394</td>\n",
       "      <td>When you see Barry Corbin in the cast of a mov...</td>\n",
       "      <td>Richard Attenborough who already given us magn...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  I only rented this because i loved the first m...   \n",
       "1           1    17393  'I don't understand. None of this makes any se...   \n",
       "2           2    17393  I know that to include everything in the book,...   \n",
       "3           3    17393  This is easily the worst, most offensive piece...   \n",
       "4           4    17394  When you see Barry Corbin in the cast of a mov...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  This film plunges headlong into the realm of t...          0.4   \n",
       "1  Taylor Hackford wanted to make this movie for ...          0.4   \n",
       "2  I don't know about the real Cobb but I got the...          0.8   \n",
       "3  The film starts in the Long Island Kennel Club...          0.4   \n",
       "4  Richard Attenborough who already given us magn...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  \n",
       "2           0.0  \n",
       "3           0.6  \n",
       "4           0.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd18eb",
   "metadata": {},
   "source": [
    "# 1.Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f8b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "df['CliChats']= df['CliChats'].str.lower()\n",
    "df['SpChats']=df['SpChats'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d94fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>i only rented this because i loved the first m...</td>\n",
       "      <td>this film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>'i don't understand. none of this makes any se...</td>\n",
       "      <td>taylor hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  i only rented this because i loved the first m...   \n",
       "1           1    17393  'i don't understand. none of this makes any se...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  this film plunges headlong into the realm of t...          0.4   \n",
       "1  taylor hackford wanted to make this movie for ...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3de02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing html tags \n",
    "import re \n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3edcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(remove_html_tags)\n",
    "df['SpChats'] = df['SpChats'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0939ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>i only rented this because i loved the first m...</td>\n",
       "      <td>this film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>'i don't understand. none of this makes any se...</td>\n",
       "      <td>taylor hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  i only rented this because i loved the first m...   \n",
       "1           1    17393  'i don't understand. none of this makes any se...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  this film plunges headlong into the realm of t...          0.4   \n",
       "1  taylor hackford wanted to make this movie for ...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d83ebb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove URLs\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efd446d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(remove_url)\n",
    "df['SpChats'] = df['SpChats'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4aeaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>i only rented this because i loved the first m...</td>\n",
       "      <td>this film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>'i don't understand. none of this makes any se...</td>\n",
       "      <td>taylor hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  i only rented this because i loved the first m...   \n",
       "1           1    17393  'i don't understand. none of this makes any se...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  this film plunges headlong into the realm of t...          0.4   \n",
       "1  taylor hackford wanted to make this movie for ...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98501e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations \n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(remove_punctuations)\n",
    "df['SpChats'] = df['SpChats'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "269f94dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>i only rented this because i loved the first m...</td>\n",
       "      <td>this film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>i dont understand none of this makes any sense...</td>\n",
       "      <td>taylor hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  i only rented this because i loved the first m...   \n",
       "1           1    17393  i dont understand none of this makes any sense...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  this film plunges headlong into the realm of t...          0.4   \n",
       "1  taylor hackford wanted to make this movie for ...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36312252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat word treatment\n",
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "def chat_word_treatment(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.lower() in abbreviations:\n",
    "            new_text.append(abbreviations[w.lower()])\n",
    "        else :\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692a5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(chat_word_treatment)\n",
    "df['SpChats'] = df['SpChats'].apply(chat_word_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "123dcbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CliChats</th>\n",
       "      <th>SpChats</th>\n",
       "      <th>CliAvgSenti</th>\n",
       "      <th>Sp_avg_senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17393</td>\n",
       "      <td>i only rented this because i loved the first m...</td>\n",
       "      <td>this film plunges headlong into the realm of t...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17393</td>\n",
       "      <td>'i don't understand. none of this makes any se...</td>\n",
       "      <td>taylor hackford wanted to make this movie for ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  OrderId                                           CliChats  \\\n",
       "0           0    17393  i only rented this because i loved the first m...   \n",
       "1           1    17393  'i don't understand. none of this makes any se...   \n",
       "\n",
       "                                             SpChats  CliAvgSenti  \\\n",
       "0  this film plunges headlong into the realm of t...          0.4   \n",
       "1  taylor hackford wanted to make this movie for ...          0.4   \n",
       "\n",
       "   Sp_avg_senti  \n",
       "0           0.4  \n",
       "1           0.6  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d65ffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spelling correction\n",
    "from textblob import TextBlob\n",
    "def spell_correct(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        textBlb = TextBlob(w)\n",
    "        new_text.append(textBlb.correct().string)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "804cc4f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCliChats\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCliChats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspell_correct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpChats\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpChats\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(spell_correct)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mspell_correct\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m      6\u001b[0m     textBlb \u001b[38;5;241m=\u001b[39m TextBlob(w)\n\u001b[0;32m----> 7\u001b[0m     new_text\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtextBlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(new_text)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:609\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m--> 609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:608\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[1;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m    609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/en/__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w]) \\\n\u001b[1;32m   1398\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w)) \\\n\u001b[0;32m-> 1399\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \\\n\u001b[1;32m   1400\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[1;32m   1401\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m   1402\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1367\u001b[0m, in \u001b[0;36mSpelling._edit1\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1364\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m   1365\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1366\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m-> 1367\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA]\n\u001b[1;32m   1368\u001b[0m )\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/textblob/_text.py:1367\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 80% is covered by edit distance 1.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Edit distance 1 = one character deleted, swapped, replaced or inserted.\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m split \u001b[38;5;241m=\u001b[39m [(w[:i], w[i:]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m   1363\u001b[0m delete, transpose, replace, insert \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1364\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[1;32m   1365\u001b[0m     [a \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1366\u001b[0m     [a \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA \u001b[38;5;28;01mif\u001b[39;00m b],\n\u001b[0;32m-> 1367\u001b[0m     [\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;241m0\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m Spelling\u001b[38;5;241m.\u001b[39mALPHA]\n\u001b[1;32m   1368\u001b[0m )\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(delete \u001b[38;5;241m+\u001b[39m transpose \u001b[38;5;241m+\u001b[39m replace \u001b[38;5;241m+\u001b[39m insert)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(spell_correct)\n",
    "df['SpChats'] = df['SpChats'].apply(spell_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62dcd02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/neelchoksi19bce0990/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b8ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords:\n",
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else :\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(remove_stopwords)\n",
    "df['SpChats'] = df['SpChats'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming:\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46861a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CliChats'] = df['CliChats'].apply(stem_words)\n",
    "df['SpChats'] = df['SpChats'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff26e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "sent1 = \"I ada ad fa dfa df!\"\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682226e",
   "metadata": {},
   "source": [
    "# 2.Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5737149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c573cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc7cd59",
   "metadata": {},
   "source": [
    "# 3.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf68a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6560f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebd024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa611554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cadf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81159f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "795e0764",
   "metadata": {},
   "source": [
    "# 4.TextClassif - ML Models\n",
    "- text vectorization : BOW, NGrams, TFIDF , word2vec\n",
    "- NB\n",
    "- Random forest\n",
    "- hyperparamter tuning : Email Spam classifier vid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd2fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc75bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a34a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e325c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc79ea9",
   "metadata": {},
   "source": [
    "# 5.own prev- hybrid ml models apply \n",
    "- last part use : https://github.com/campusx-official/sms-spam-classifier/blob/main/sms-spam-detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce80ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dffb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1f2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e1fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f349424f",
   "metadata": {},
   "source": [
    "# 6.pydata - london : ANN , LSTM , GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15ec0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d00d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e89cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6d7c14",
   "metadata": {},
   "source": [
    "# 7.pytorch - ensemble use on ANN , LSTM , GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fe413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
